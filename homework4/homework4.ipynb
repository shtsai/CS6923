{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CS6923 Machine Learning\n",
    "### Homework 4\n",
    "### Shang-Hung Tsai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\n",
    "(a)\n",
    "The dataset with 3 positive exampls and 5 negative examples will have higher entropy, because there is more uncertainty in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Entropy(S) = - (\\frac{3}{6} log_2 {\\frac{3}{6}} + \\frac{3}{6} log_2 {\\frac{3}{6}}) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{3}{6}*-(\\frac{2}{3} log_2 {\\frac{2}{3}} + \\frac{1}{3} log_2 {\\frac{1}{3}})) + (\\frac{3}{6}*-(\\frac{1}{3} log_2 {\\frac{1}{3}} + \\frac{2}{3} log_2 {\\frac{2}{3}})) \\\\ \n",
    "= (\\frac{1}{2} * 0.9183) + (\\frac{1}{2} * 0.9183) = 0.9183$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Information-Gain(S) = Entropy(S) - \\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = 1 - 0.9183 = 0.0817$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)  \n",
    "First Decision:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{3}{6}*-(\\frac{2}{3} log_2 {\\frac{2}{3}} + \\frac{1}{3} log_2 {\\frac{1}{3}})) + (\\frac{3}{6}*-(\\frac{1}{3} log_2 {\\frac{1}{3}} + \\frac{2}{3} log_2 {\\frac{2}{3}})) \\\\ \n",
    "= (\\frac{1}{2} * 0.9183) + (\\frac{1}{2} * 0.9183) = 0.9183$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_2$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{2}{6}*-(\\frac{1}{2} log_2 {\\frac{1}{2}} + \\frac{1}{2} log_2 {\\frac{1}{2}})) + (\\frac{4}{6}*-(\\frac{2}{4} log_2 {\\frac{2}{4}} + \\frac{2}{4} log_2 {\\frac{2}{4}})) \\\\ \n",
    "= (\\frac{1}{3} * 0.9183) + (\\frac{2}{3} * 1) = 0.9728 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_3$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{2}{6}*-(\\frac{1}{2} log_2 {\\frac{1}{2}} + \\frac{1}{2} log_2 {\\frac{1}{2}})) + (\\frac{4}{6}*-(\\frac{2}{4} log_2 {\\frac{2}{4}} + \\frac{2}{4} log_2 {\\frac{2}{4}})) \\\\ \n",
    "= (\\frac{1}{3} * 1) + (\\frac{2}{3} * 1) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will use $x_1$ on the first decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_1$ is T, the remaining data is:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>      \n",
    "        <th>$x_2$</th>\n",
    "        <th>$x_3$</th>        \n",
    "        <th>$r$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>$x^{(1)}$</th>    \n",
    "        <th>$F$</th>\n",
    "        <th>$F$</th>        \n",
    "        <th>$+$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>$x^{(3)}$</th>    \n",
    "        <th>$T$</th>\n",
    "        <th>$F$</th>        \n",
    "        <th>$-$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>$x^{(4)}$</th>      \n",
    "        <th>$F$</th>\n",
    "        <th>$T$</th>        \n",
    "        <th>$+$</th>\n",
    "    </tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Decision when $x_1$ is T:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_2$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{1}{3}*-(0 * log_2 {0} + 1 * log_2 {1})) + (\\frac{2}{3}*-(\\frac{2}{2} log_2 {\\frac{2}{2}} + 0 * log_2 {0})) \\\\ \n",
    "= 0 + 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_3$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{1}{3}*-(\\frac{1}{1} log_2 {\\frac{1}{1}} + 0 * log_2 {0})) + (\\frac{2}{3}*-(\\frac{1}{2} log_2 {\\frac{1}{2}} + \\frac{1}{2} log_2 {\\frac{1}{2}})) \\\\ \n",
    "= 0 + (\\frac{2}{3} * 1) = 0.6667 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will use $x_2$ as the second desicion when $x_1$ is T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $x_1$ is F, the remaining data is:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>       \n",
    "        <th>$x_2$</th>\n",
    "        <th>$x_3$</th>        \n",
    "        <th>$r$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>$x^{(2)}$</th>        \n",
    "        <th>$T$</th>\n",
    "        <th>$F$</th>        \n",
    "        <th>$+$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>$x^{(5)}$</th>  \n",
    "        <th>$F$</th>\n",
    "        <th>$T$</th>        \n",
    "        <th>$-$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>$x^{(6)}$</th>  \n",
    "        <th>$F$</th>\n",
    "        <th>$F$</th>        \n",
    "        <th>$-$</th>\n",
    "    </tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Decision when $x_1$ is F:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_2$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{1}{3}*-(\\frac{1}{1} * log_2 {\\frac{1}{1}} + 0 * log_2 {0})) + (\\frac{2}{3}*-(0 * log_2 {0} + \\frac{2}{2} log_2 {\\frac{2}{2}})) \\\\ \n",
    "= 0 + 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_3$: $$\\sum_{v \\in V}{\\frac{|S_v|}{|S|}Entropy(S_v)} = (\\frac{1}{3}*-(0 * log_2 {0} + \\frac{1}{1} log_2 {\\frac{1}{1}})) + (\\frac{2}{3}*-(\\frac{1}{2} log_2 {\\frac{1}{2}} + \\frac{1}{2} log_2 {\\frac{1}{2}})) \\\\ \n",
    "= 0 + (\\frac{2}{3} * 1) = 0.6667 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will use $x_2$ as the second desicion when $x_1$ is F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above decision tree achieves 0% training error on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tree is:\n",
    "\n",
    "                                [X_1]\n",
    "                                /   \\\n",
    "                            T  /     \\  F\n",
    "                          [x_2]      [x_2] \n",
    "                           / \\        / \\\n",
    "                        T /   \\ F   T/   \\ F\n",
    "                 r       -     +    +     -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X) = -(\\frac{3}{6} log_2{\\frac{3}{6}} + \\frac{3}{6} log_2{\\frac{3}{6}}) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(Y|X) = \\frac{3}{6}*(-\\frac{2}{3}log_2{\\frac{2}{3}}-\\frac{1}{3}log_2{\\frac{1}{3}}) + \\frac{3}{6}*(-\\frac{1}{3}log_2{\\frac{1}{3}}-\\frac{2}{3}log_2{\\frac{2}{3}}) \\\\\n",
    "= 0.9183 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(X) - H(Y|X) = 1 - 0.9183 = 0.0817 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)  \n",
    "The first attribute is likely to have a higher information gain because it has a much larger number of possible values (i.e. $10^9$), while the other attribute only has two possible values.  \n",
    "\n",
    "We would not want to place the first attribute into a node in our decision tree because the 9-digit driver's license number has no real meanings in this classification problem. If we use this attribute, the decision tree will just memorize the training data, and it leads to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\n",
    "(a)\n",
    "$$100 * 500 = 50000$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. \n",
    "\n",
    "For $i \\in \\{1, 2\\}$:\n",
    "$$\\Delta v_{ih} = - \\eta\\frac{\\partial E(W, v|X)}{\\partial v_{ih}} \\\\\n",
    "= - \\eta\\frac{\\partial}{\\partial v_{ih}} \\frac{1}{2}\\big[ (r_1-y_1)^2 + (r_2-y_2)^2 + 5(r_3-y_3)^2 \\big] \\\\\n",
    "= - \\eta\\frac{\\partial}{\\partial v_{ih}} \\frac{1}{2}(r_1-y_1)^2 + \\frac{1}{2}(r_2-y_2)^2 + \\frac{5}{2}(r_3-y_3)^2 \\\\\n",
    "= - \\eta (r_i-y_i)(-\\frac{\\partial y_i}{\\partial v_{ih}}) \\\\\n",
    "= \\eta (r_i-y_i)z_h\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i = 3$:\n",
    "$$\\Delta v_{3h} = - \\eta\\frac{\\partial E(W, v|X)}{\\partial v_{3h}} \\\\\n",
    "= - \\eta\\frac{\\partial}{\\partial v_{3h}} \\frac{1}{2}\\big[ (r_1-y_1)^2 + (r_2-y_2)^2 + 5(r_3-y_3)^2 \\big] \\\\\n",
    "= - \\eta\\frac{\\partial}{\\partial v_{3h}} \\frac{1}{2}(r_1-y_1)^2 + \\frac{1}{2}(r_2-y_2)^2 + \\frac{5}{2}(r_3-y_3)^2 \\\\\n",
    "= - \\eta 5(r_3-y_3)(-\\frac{\\partial y_3}{\\partial v_{3h}}) \\\\\n",
    "= 5\\eta (r_3-y_3)z_h\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.  \n",
    "$$\\Delta w_{hj} = - \\eta\\frac{\\partial E}{\\partial W_{hj}} \\\\\n",
    "= - \\eta \\Big(\\frac{\\partial E}{\\partial y_1}\\frac{\\partial y_1}{\\partial z_h}\\frac{\\partial z_h}{\\partial w_{hj}} + \\frac{\\partial E}{\\partial y_2}\\frac{\\partial y_2}{\\partial z_h}\\frac{\\partial z_h}{\\partial w_{hj}} + \\frac{\\partial E}{\\partial y_3}\\frac{\\partial y_3}{\\partial z_h}\\frac{\\partial z_h}{\\partial w_{hj}} \\Big) \\\\\n",
    "= - \\eta \\Big( -(r_1 - y_1)v_{1h}z_h(1-z_h)x_j - (r_2 - y_2)v_{2h}z_h(1-z_h)x_j - 5(r_3 - y_3)v_{3h}z_h(1-z_h)x_j  \\Big) \\\\\n",
    "= \\eta \\Big( (r_1 - y_1)v_{1h}z_h(1-z_h)x_j + (r_2 - y_2)v_{2h}z_h(1-z_h)x_j + 5(r_3 - y_3)v_{3h}z_h(1-z_h)x_j  \\Big) \\\\\n",
    "= \\eta \\Big( (r_1 - y_1)v_{1h} + (r_2 - y_2)v_{2h} + 5(r_3 - y_3)v_{3h}  \\Big) z_h(1-z_h)x_j  \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  \n",
    "(a) By using NeuralNetRZeroOne, we can make sure that each output value is within the interval\\[0,1\\]. On the other hand, NerualNetRK does not guarantee that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) We use NeuralNetZeroOne because this is a regression problem, and we will minimize the squared error. NeuralNetCK is not appropriate here because it solves classification problem and it uses cross-entropy as the error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)  \n",
    "i.a.  \n",
    "epoch 10: err 0.175075  \n",
    "epoch 20: err 0.168263  \n",
    "epoch 30: err 0.166156  \n",
    "epoch 50: err 0.162933  \n",
    "The error is decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.b.  \n",
    "epoch 4999: err 0.000030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.c.  \n",
    "$y_1 = 0.007032$  \n",
    "$y_2 = 0.992715$  \n",
    "$y_3 = 0.992655$  \n",
    "$y_4 = 0.008992$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.  \n",
    "epoch 0: err 0.112072  \n",
    "epoch 9999: err 0.000012  \n",
    "\n",
    "4.57766073e-09  \n",
    "5.30416630e-06  \n",
    "1.53758109e-02  \n",
    "6.82366619e-04  \n",
    "2.72965829e-01  \n",
    "9.26995318e-08  \n",
    "4.76270065e-03  \n",
    "8.48012010e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.  \n",
    "epoch 0: err 0.030797  \n",
    "epoch 3999: err 0.000167  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv.  \n",
    "The code is implementing stochastic gradient descent because it updates weights after processing each input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v.  \n",
    "The arithmetic expression \"x/np.pi\" is being used to scale the inputs to be between -1 and 1.  \n",
    "The expression \"(np.cos(x) + 1)/2\" is being used to scale the outputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output values were also scaled because the range of original cosine function is between -1 and +1, while the output of neural net is between 0 and 1 (because of sigmoid function). By using the above scaled expression, the cosine values were shifted to range between 0 and 1.  \n",
    "The neural net has difficulty achieving small error with the unscaled output because the sigmoid function at output node can only produce value between 0 and 1. It is impossible to approximate the original cosine function. With scaled output, all values are between 0 and 1, and the neural net can achieve small error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  \n",
    "(a)  \n",
    "$k^2-3k-28$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  \n",
    "By solving $k^2-3k-28 = 0$, we get:  \n",
    "$k_1 = 7$  \n",
    "$k_2 = -4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)  \n",
    "For $k=7$, we get:  \n",
    "$x-2y = 0$  \n",
    "$x = 2y$  \n",
    "Since $\\sqrt{x^2+y^2} = 1$  \n",
    "$5y^2 = 1$  \n",
    "So $y = \\sqrt{\\frac{1}{5}} = 0.4472$  \n",
    "$x = 2y = 0.8944$  \n",
    "Eigenvector is \\[0.8944, 0.4472\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k=-4$, we get:  \n",
    "$2x + 7y = 0$  \n",
    "$x = -\\frac{7}{2}y$  \n",
    "Since $\\sqrt{x^2+y^2} = 1$  \n",
    "$\\frac{53}{4}y^2 = 1$  \n",
    "So $y = \\sqrt{\\frac{4}{53}} = 0.2747$  \n",
    "$x = -\\frac{7}{2}y = -0.9615$  \n",
    "Eigenvector is \\[-0.9615, 0.2747\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)  \n",
    "By using linalg.eig() in Python, I get:  \n",
    "$k_1 = -4$  \n",
    "$v_1 = [-0.9615, 0.2747]$  \n",
    "$k_2 = 7$  \n",
    "$v_2 = [-0.8944, -0.4472]$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Python solution for eigenvector for k=7 is different by two negative signs, but both are valid solution with $L_2$ norm equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  \n",
    "(a)  \n",
    "$$B = \n",
    "\\begin{bmatrix}\n",
    "    -0.75 & -1.5 & 0  \\\\\n",
    "    3.25  & 2.5  & 0  \\\\\n",
    "    1.25  & -2.5 & -2 \\\\\n",
    "    -3.75 & 1.5  & 2 \n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  \n",
    "$$s_{1,2} = \\frac{(-0.75)*(-1.5)+3.25*2.5+1.25*(-2.5)-3.75*1.5 }{4-1}=\\frac{0.5}{3} = 0.1667 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)  \n",
    "The sample covariance of $x_1$ and $x_2$ computed using Python is also 0.1667  \n",
    "  \n",
    "The largest eigenvalue of the sameple covariance matrix of B is 10.5561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)  \n",
    "First column: \\[0.33956422, -2.31212882, -2.49664663, 4.46921123\\]  \n",
    "Second column: \\[-1.598666, 3.38610763, -2.34865846, 0.56121683\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  \n",
    "(a)  \n",
    "```\n",
    "plt_face(fea[4])\n",
    "```  \n",
    "![fifth_face](6a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  \n",
    "```\n",
    "meanface = np.mean(fea, axis=0)\n",
    "plt_face(meanface)\n",
    "```  \n",
    "![mean_face](6b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)  \n",
    "Command I used:  \n",
    "```\n",
    "import sklearn.decomposition as skd\n",
    "pca = skd.PCA(n_components=6)\n",
    "skd.PCA.fit(pca, fea)\n",
    "newfea = pca.transform(fea)\n",
    "print(newfea[4])\n",
    "```  \n",
    "The values of the first 6 attributes of the fifth image are  \n",
    "$[547.9159, 811.6605, 592.5791, 268.08398, 171.3558, -17.509369]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)  \n",
    "For the first 6 principal components, the command I used is:  \n",
    "```\n",
    "# 6 principal components\n",
    "pca = skd.PCA(n_components=6)\n",
    "skd.PCA.fit(pca, fea)\n",
    "Z = pca.transform(fea)\n",
    "W = pca.components_\n",
    "X6 = W.T.dot(Z.T) + np.array([meanface]).T\n",
    "plt_face(X6.T[4])\n",
    "```  \n",
    "![reconstructed_face_6](6d6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first 100 principal components, the command I used is:  \n",
    "```\n",
    "# 100 principal components\n",
    "pca = skd.PCA(n_components=100)\n",
    "skd.PCA.fit(pca, fea)\n",
    "Z = pca.transform(fea)\n",
    "W = pca.components_\n",
    "X100 = W.T.dot(Z.T) + np.array([meanface]).T\n",
    "plt_face(X100.T[4])\n",
    "```  \n",
    "![reconstructed_face_100](6d100.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
