{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CS6923 Machine Learning\n",
    "### Homework 3\n",
    "### Shang-Hung Tsai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\n",
    "(a)\n",
    "$$ P(C_1 | x^{(1)})) = \\frac {1}{1+e^{-(0.01*(-5)+0.01*3+0.01)}} = \\frac {1}{1+e^{-0.01}} = 0.4975 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) The predicted label is $C_2$ or 0 because 0.4975 < 0.5. This prediction is inconsistent with the label of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(C_1| x^{(2)})) = \\frac {1}{1+e^{-(0.01*2+0.01*3+0.01)}} = \\frac {1}{1+e^{-0.06}} = 0.515 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict label is $C_1$ or 1 because 0.515 > 0.5. This prediction is inconsistent with the label of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)\n",
    "$$Err(w, w_0 | X) = -(1 * log(0.4975) + 1*log(1-0.515))=1.4218$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f)\n",
    "$$w_0 = 0.01+0.005*(1-0.4975+0-0.515)=0.0099375$$\n",
    "$$w_1 = 0.01+0.005*((1-0.4975)*(-5)+(0-0.515)*2)=-0.0077125$$\n",
    "$$w_2 = 0.01+0.005*((1-0.4975)*3+(0-0.515)*3)=0.0098125$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g)\n",
    "$$ P(C_1 | x^{(1)})) = \\frac {1}{1+e^{-(-0.0077125*(-5)+0.0098125*3+0.0099375)}} = 0.519475 $$\n",
    "$$ P(C_1 | x^{(2)})) = \\frac {1}{1+e^{-(-0.0077125*2+0.0098125*3+0.0099375)}} = 0.505987 $$\n",
    "$$Err(w, w_0 | X) = -(1 * log(0.519475) + 1*log(1-0.505987))=1.360130$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) The cross-entropy went down after one iteration of the gradient descent. This is expected, because gradient descent should gradually improve the weights of the function and reduce error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) The learning rate could be too high, and therefore causes oscillations and divergence. <br>\n",
    "Suggestion: decrease learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) The learning rate could be too low, and therefore convergence becomes very slow. <br>\n",
    "Suggestion: increase learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) The value could be stucked at a local minimum. <br>\n",
    "Suggestion: try run gradient descent again with a different initial value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to predict class $C_1$ if $h(x) > 0.30$,\n",
    "we have\n",
    "$$\\frac{1}{1+e^{-(w^T x+w_0)}} > 0.3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1 > 0.3+0.3e^{-(w^T x+w_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{7}{3} > e^{-(w^T x+w_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take natural log on both side of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ln(\\frac{7}{3}) > -(w^T x+w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(w^T x+w_0) > -ln(\\frac{7}{3}) = -0.847298$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can predict positive if $g(x) > -0.847298$ and negative otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original update function, the updated term is obtained by taking partial derivatives on the cross-entropy function\n",
    "$$\\delta w_j = - \\eta \\frac{\\partial Err}{\\partial w_j} = - \\eta (-\\sum_t (r^t-y^t)x_j) = \\eta \\sum_t (r^t-y^t)x_j $$ \n",
    "for j = 1,...d\n",
    "\n",
    "$$\\delta w_j = - \\eta \\frac{\\partial Err}{\\partial w_j} = - \\eta (-\\sum_t (r^t-y^t)) = \\eta \\sum_t (r^t-y^t) $$ \n",
    "for j = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use regularized version of the cross-entropy error function, we will need to also take partial derivatives on the regularized term, which gives us $\\lambda w_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we get\n",
    "$$\\delta w_j = - \\eta \\frac{\\partial Err}{\\partial w_j} = - \\eta (-\\sum_t \\Big[(r^t-y^t)x_j\\Big] + \\lambda w_j) = \\eta (\\sum_t \\Big[(r^t-y^t)x_j\\Big] -\\lambda w_j) $$ \n",
    "for j = 1,...d\n",
    "\n",
    "$$\\delta w_j = - \\eta \\frac{\\partial Err}{\\partial w_j} = - \\eta (-\\sum_t (r^t-y^t)) = \\eta \\sum_t (r^t-y^t) $$ \n",
    "for j = 0\n",
    "\n",
    "$w_0$ does not change because regularized term does not include $w_0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
